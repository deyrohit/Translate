{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.3-cp312-cp312-win_amd64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.11-cp312-cp312-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.10-cp312-cp312-win_amd64.whl.metadata (8.6 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Using cached preshed-3.0.9-cp312-cp312-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.0 (from spacy)\n",
      "  Downloading thinc-8.3.3-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.0-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Using cached weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.15.1-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\ct_user\\anaconda3\\envs\\translation\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\ct_user\\anaconda3\\envs\\translation\\lib\\site-packages (from spacy) (2.1.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\ct_user\\anaconda3\\envs\\translation\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.10.4-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ct_user\\anaconda3\\envs\\translation\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ct_user\\anaconda3\\envs\\translation\\lib\\site-packages (from spacy) (72.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ct_user\\anaconda3\\envs\\translation\\lib\\site-packages (from spacy) (24.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.27.2-cp312-cp312-win_amd64.whl.metadata (6.7 kB)\n",
      "Collecting typing-extensions>=4.12.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ct_user\\anaconda3\\envs\\translation\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ct_user\\anaconda3\\envs\\translation\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ct_user\\anaconda3\\envs\\translation\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ct_user\\anaconda3\\envs\\translation\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
      "Collecting blis<1.2.0,>=1.1.0 (from thinc<8.4.0,>=8.3.0->spacy)\n",
      "  Downloading blis-1.1.0-cp312-cp312-win_amd64.whl.metadata (7.9 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.0->spacy)\n",
      "  Using cached confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\ct_user\\anaconda3\\envs\\translation\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Collecting click>=8.0.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.20.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ct_user\\anaconda3\\envs\\translation\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ct_user\\anaconda3\\envs\\translation\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading wrapt-1.17.0-cp312-cp312-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading spacy-3.8.3-cp312-cp312-win_amd64.whl (11.8 MB)\n",
      "   ---------------------------------------- 0.0/11.8 MB ? eta -:--:--\n",
      "   ---------------------------------------  11.5/11.8 MB 71.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.8/11.8 MB 36.8 MB/s eta 0:00:00\n",
      "Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.10-cp312-cp312-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.11-cp312-cp312-win_amd64.whl (25 kB)\n",
      "Using cached preshed-3.0.9-cp312-cp312-win_amd64.whl (122 kB)\n",
      "Downloading pydantic-2.10.4-py3-none-any.whl (431 kB)\n",
      "Downloading pydantic_core-2.27.2-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 36.7 MB/s eta 0:00:00\n",
      "Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.0-cp312-cp312-win_amd64.whl (632 kB)\n",
      "   ---------------------------------------- 0.0/632.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 632.6/632.6 kB 24.7 MB/s eta 0:00:00\n",
      "Downloading thinc-8.3.3-cp312-cp312-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 15.3 MB/s eta 0:00:00\n",
      "Downloading typer-0.15.1-py3-none-any.whl (44 kB)\n",
      "Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading blis-1.1.0-cp312-cp312-win_amd64.whl (6.4 MB)\n",
      "   ---------------------------------------- 0.0/6.4 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 2.9/6.4 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 3.4/6.4 MB 15.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 3.4/6.4 MB 15.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 3.4/6.4 MB 15.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 3.4/6.4 MB 15.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 3.4/6.4 MB 15.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 3.4/6.4 MB 15.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 3.4/6.4 MB 15.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 3.4/6.4 MB 15.5 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 3.9/6.4 MB 1.8 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 4.5/6.4 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 4.5/6.4 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 4.5/6.4 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 4.5/6.4 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 4.5/6.4 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 4.7/6.4 MB 1.4 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 5.0/6.4 MB 1.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 5.2/6.4 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 5.5/6.4 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.0/6.4 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.3/6.4 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.4/6.4 MB 1.4 MB/s eta 0:00:00\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading cloudpathlib-0.20.0-py3-none-any.whl (52 kB)\n",
      "Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/5.4 MB 3.4 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 1.3/5.4 MB 3.7 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 2.4/5.4 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 3.4/5.4 MB 4.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 3.9/5.4 MB 4.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 3.9/5.4 MB 4.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 3.9/5.4 MB 4.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 3.9/5.4 MB 4.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 4.2/5.4 MB 2.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 5.2/5.4 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 2.6 MB/s eta 0:00:00\n",
      "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading marisa_trie-1.2.1-cp312-cp312-win_amd64.whl (150 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading wrapt-1.17.0-cp312-cp312-win_amd64.whl (38 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: cymem, wrapt, wasabi, typing-extensions, spacy-loggers, spacy-legacy, shellingham, murmurhash, mdurl, marisa-trie, cloudpathlib, click, catalogue, blis, annotated-types, srsly, smart-open, pydantic-core, preshed, markdown-it-py, language-data, rich, pydantic, langcodes, typer, confection, weasel, thinc, spacy\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.11.0\n",
      "    Uninstalling typing_extensions-4.11.0:\n",
      "      Successfully uninstalled typing_extensions-4.11.0\n",
      "Successfully installed annotated-types-0.7.0 blis-1.1.0 catalogue-2.0.10 click-8.1.8 cloudpathlib-0.20.0 confection-0.1.5 cymem-2.0.10 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.11 preshed-3.0.9 pydantic-2.10.4 pydantic-core-2.27.2 rich-13.9.4 shellingham-1.5.4 smart-open-7.1.0 spacy-3.8.3 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.0 thinc-8.3.3 typer-0.15.1 typing-extensions-4.12.2 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch 2.5.1 requires sympy==1.13.1; python_version >= \"3.9\", but you have sympy 1.13.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import spacy\n",
    "from transformer import Transformer \n",
    "import torch.optim as optim\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m nlp_en \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men_core_web_sm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m nlp_fr \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfr_core_news_sm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\CT_USER\\anaconda3\\envs\\translation\\Lib\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\CT_USER\\anaconda3\\envs\\translation\\Lib\\site-packages\\spacy\\util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp_en = spacy.load('en_core_web_sm')\n",
    "nlp_fr = spacy.load('fr_core_news_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m nlp_en \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men_core_web_sm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m nlp_fr \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfr_core_news_sm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\CT_USER\\anaconda3\\envs\\translation\\Lib\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\CT_USER\\anaconda3\\envs\\translation\\Lib\\site-packages\\spacy\\util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "nlp_en = spacy.load('en_core_web_sm')\n",
    "nlp_fr = spacy.load('fr_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre Process with punctuatuion and spaces\n",
    "def preprocess_text_spacy(text, nlp_model):\n",
    "\n",
    "    doc = nlp_model(text)\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if not token.is_punct and not token.is_space:\n",
    "            tokens.append(token.text.lower())\n",
    "    return tokens\n",
    "\n",
    "# Tokens to IDs\n",
    "def tokens_to_ids(tokens, vocab):\n",
    "    return [vocab.get(token, vocab['<UNK>']) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('src_vocab.json', 'r') as src_file:\n",
    "    src_vocab = json.load(src_file)\n",
    "\n",
    "with open('tgt_vocab.json', 'r') as tgt_file:\n",
    "    tgt_vocab = json.load(tgt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = len(src_vocab)\n",
    "tgt_vocab_size = len(tgt_vocab)\n",
    "d_model = 512\n",
    "heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 100 \n",
    "dropout = 0.1\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Transformer(src_vocab_size, tgt_vocab_size, d_model, heads, num_layers, d_ff, max_seq_length, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder_embedding): Embedding(16317, 512)\n",
       "  (decoder_embedding): Embedding(29592, 512)\n",
       "  (positional_encoding): PositionalEncoding()\n",
       "  (encoder_layers): ModuleList(\n",
       "    (0-5): 6 x EncoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (feed_forward): PositionWiseFeedForward(\n",
       "        (fcnn1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fcnn2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder_layers): ModuleList(\n",
       "    (0-5): 6 x DecoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (cross_attn): MultiHeadAttention(\n",
       "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (feed_forward): PositionWiseFeedForward(\n",
       "        (fcnn1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fcnn2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=512, out_features=29592, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#             ignore index = 0-> Handing Padding Tokens\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CT_USER\\AppData\\Local\\Temp\\ipykernel_20396\\3011260972.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('Model.pth', map_location=torch.device('cpu'))\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('Model.pth', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, src_tensor, max_len, device, id_to_word):\n",
    "    model.eval()\n",
    "    tgt_tokens = [src_vocab['<SOS>']]  \n",
    "    tgt_tensor = torch.tensor([tgt_tokens]).to(device)\n",
    "    \n",
    "    predictions = []\n",
    " \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            output = model(src_tensor, tgt_tensor)\n",
    "            next_token = output.argmax(dim=-1)[:, -1].item()\n",
    "            predictions.append(next_token)\n",
    "           \n",
    "            tgt_tokens.append(next_token)\n",
    "            tgt_tensor = torch.tensor([tgt_tokens]).to(device)  \n",
    " \n",
    "    predicted_words = [id_to_word.get(token_id, '<UNK>') for token_id in predictions]\n",
    "    predicted_sentence = ' '.join(predicted_words)\n",
    "    return predicted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predicted_sentence(model, src_sentence, src_vocab, tgt_vocab, preprocess, nlp, predict, device, max_len=None):\n",
    "    num = src_sentence.count(' ')+2\n",
    "    id_to_word = {id_: word for word, id_ in tgt_vocab.items()}\n",
    "    src_tensor = torch.tensor(tokens_to_ids(preprocess(src_sentence, nlp), src_vocab)).unsqueeze(0).to(device)\n",
    "    predicted_sentence = predict(model, src_tensor, max_len=num, device=device, id_to_word=id_to_word)\n",
    "    a=0\n",
    "    if (predicted_sentence.__contains__('<UNK>')):\n",
    "        a=a+1\n",
    "    predicted_sentence=predicted_sentence.replace('<UNK>','')\n",
    "    \n",
    "    def get_prediction(predicted_sentence, num, a):\n",
    "        words = predicted_sentence.split()  \n",
    "        return ' '.join(words[:num + a]) \n",
    "    ans=get_prediction(predicted_sentence, num, a)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: I am going there\n",
      "Predicted translation: je vais y aller\n"
     ]
    }
   ],
   "source": [
    "src_sentence = \"I am going there\"\n",
    "predicted_sentence = generate_predicted_sentence(model=model,src_sentence=src_sentence,src_vocab=src_vocab,tgt_vocab=tgt_vocab,preprocess=preprocess_text_spacy,\n",
    "                                                    nlp=nlp_en,predict=predict,device=device)\n",
    "print(\"Input Sentence:\" ,src_sentence)\n",
    "print(f'Predicted translation: {predicted_sentence}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: I am having fun on Friday\n",
      "Predicted translation: bien vendredi je amuse vendredi vendredi\n"
     ]
    }
   ],
   "source": [
    "src_sentence = \"I am having fun on Friday\"\n",
    "predicted_sentence = generate_predicted_sentence(model=model,src_sentence=src_sentence,src_vocab=src_vocab,tgt_vocab=tgt_vocab,preprocess=preprocess_text_spacy,\n",
    "                                                    nlp=nlp_en,predict=predict,device=device)\n",
    "print(\"Input Sentence:\" ,src_sentence)\n",
    "\n",
    "print(f'Predicted translation: {predicted_sentence}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: I am here\n",
      "Predicted translation: je suis ici\n"
     ]
    }
   ],
   "source": [
    "src_sentence = \"I am here\"\n",
    "predicted_sentence = generate_predicted_sentence(model=model,src_sentence=src_sentence,src_vocab=src_vocab,tgt_vocab=tgt_vocab,preprocess=preprocess_text_spacy,\n",
    "                                                    nlp=nlp_en,predict=predict,device=device)\n",
    "print(\"Input Sentence:\" ,src_sentence)\n",
    "print(f'Predicted translation: {predicted_sentence}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: I came from college\n",
      "Predicted translation: un coll√®ge de\n"
     ]
    }
   ],
   "source": [
    "src_sentence = \"I came from college\"\n",
    "predicted_sentence = generate_predicted_sentence(model=model,src_sentence=src_sentence,src_vocab=src_vocab,tgt_vocab=tgt_vocab,preprocess=preprocess_text_spacy,\n",
    "                                                    nlp=nlp_en,predict=predict,device=device)\n",
    "print(\"Input Sentence:\" ,src_sentence)\n",
    "print(f'Predicted translation: {predicted_sentence}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: My name is Rohit\n",
      "Predicted translation: mon nom est bleu\n"
     ]
    }
   ],
   "source": [
    "src_sentence = \"My name is Rohit\"\n",
    "predicted_sentence = generate_predicted_sentence(model=model,src_sentence=src_sentence,src_vocab=src_vocab,tgt_vocab=tgt_vocab,preprocess=preprocess_text_spacy,\n",
    "                                                    nlp=nlp_en,predict=predict,device=device)\n",
    "print(\"Input Sentence:\" ,src_sentence)\n",
    "print(f'Predicted translation: {predicted_sentence}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: I lives in India\n",
      "Predicted translation: inde je suis inde\n"
     ]
    }
   ],
   "source": [
    "src_sentence = \"I lives in India\"\n",
    "predicted_sentence = generate_predicted_sentence(model=model,src_sentence=src_sentence,src_vocab=src_vocab,tgt_vocab=tgt_vocab,preprocess=preprocess_text_spacy,\n",
    "                                                    nlp=nlp_en,predict=predict,device=device)\n",
    "print(\"Input Sentence:\" ,src_sentence)\n",
    "print(f'Predicted translation: {predicted_sentence}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: I coding right now\n",
      "Predicted translation: tout de suite je suis\n"
     ]
    }
   ],
   "source": [
    "src_sentence = \"I coding right now\"\n",
    "predicted_sentence = generate_predicted_sentence(model=model,src_sentence=src_sentence,src_vocab=src_vocab,tgt_vocab=tgt_vocab,preprocess=preprocess_text_spacy,\n",
    "                                                    nlp=nlp_en,predict=predict,device=device)\n",
    "print(\"Input Sentence:\" ,src_sentence)\n",
    "print(f'Predicted translation: {predicted_sentence}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: I am playing cricket on field\n",
      "Predicted translation: ai du cricket sur le champ\n"
     ]
    }
   ],
   "source": [
    "src_sentence = \"I am playing cricket on field\"\n",
    "predicted_sentence = generate_predicted_sentence(model=model,src_sentence=src_sentence,src_vocab=src_vocab,tgt_vocab=tgt_vocab,preprocess=preprocess_text_spacy,\n",
    "                                                    nlp=nlp_en,predict=predict,device=device)\n",
    "print(\"Input Sentence:\" ,src_sentence)\n",
    "print(f'Predicted translation: {predicted_sentence}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: I am playing cricket\n",
      "Predicted translation: ai du cricket je\n"
     ]
    }
   ],
   "source": [
    "src_sentence = \"I am playing cricket\"\n",
    "predicted_sentence = generate_predicted_sentence(model=model,src_sentence=src_sentence,src_vocab=src_vocab,tgt_vocab=tgt_vocab,preprocess=preprocess_text_spacy,\n",
    "                                                    nlp=nlp_en,predict=predict,device=device)\n",
    "print(\"Input Sentence:\" ,src_sentence)\n",
    "print(f'Predicted translation: {predicted_sentence}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "translation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
